"""Streamlit ì•±ì˜ ì§„ì…ì  ëª¨ë“ˆ."""
import streamlit as st
from dotenv import load_dotenv
from langchain import hub
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore

st.set_page_config(page_title="ì†Œë“ì„¸ ì±—ë´‡", page_icon='ğŸ¤–')
st.title("ğŸ¤– ì†Œë“ì„¸ ì±—ë´‡")
st.caption("ì†Œë“ì„¸ì— ê´€ë ¨ëœ ëª¨ë“ ê²ƒì„ ë‹µí•´ë“œë¦½ë‹ˆë‹¤!")

# Initialize chat history
if "message_list" not in st.session_state:
    st.session_state.message_list = [{"role": "assistant", "content": "ë¬´ì—‡ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?"}]

# Display chat messages from history on app rerun
for message in st.session_state.message_list:
    with st.chat_message(message["role"]):
        st.write(message["content"])

def get_ai_message(user_message):
    """ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°›ì•„ RAG ì²´ì¸ì„ í†µí•´ ì‘ë‹µì„ ìƒì„±í•´ ë°˜í™˜í•œë‹¤.

    Parameters:
        user_message (str): ì‚¬ìš©ì ì…ë ¥ ì§ˆë¬¸.

    Returns:
        Any: LangChain ì²´ì¸ì˜ invoke ê²°ê³¼.
    """

    load_dotenv()

    embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
    index_name = 'text-markdown-index'
    database = PineconeVectorStore.from_existing_index(index_name, embeddings)

    llm = ChatOpenAI(model='gpt-4o')
    prompt = hub.pull("rlm/rag-prompt")
    retriver = database.as_retriever(search_kwargs={'k': 4})

    dictionary = ["ì‚¬ëŒì„ ë‚˜íƒ€ë‚´ëŠ” í‘œí˜„ -> ê±°ì£¼ì"]

    prompt = ChatPromptTemplate.from_template(f"""
      ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³´ê³ , ìš°ë¦¬ì˜ ì‚¬ì „ì„ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³€ê²½í•´ì£¼ì„¸ìš”.
      ë§Œì•½ ë³€ê²½í•  í•„ìš”ê°€ ì—†ë‹¤ê³  íŒë‹¨ëœë‹¤ë©´, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³€ê²½í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.
      ê·¸ëŸ° ê²½ìš° ì§ˆë¬¸ë§Œ ë¦¬í„´í•´ì£¼ì„¸ìš”
      ì‚¬ì „: {dictionary}

      ì§ˆë¬¸: {{question}}
    """)

    retrieval_qa_chat_prompt = hub.pull("langchain-ai/retrieval-qa-chat")

    combine_docs_chain = create_stuff_documents_chain(llm, retrieval_qa_chat_prompt)
    qa_chain = create_retrieval_chain(retriver, combine_docs_chain)

    dictionary_chain = prompt | llm | StrOutputParser()
    tax_chain = { "input": dictionary_chain } | qa_chain
    response_message = tax_chain.invoke({ "question": user_message })

    return response_message["answer"]


# Accept user input
if user_question := st.chat_input("ì†Œë“ì„¸ì— ê´€ë ¨ëœ ê¶ê¸ˆí•œ ë‚´ìš©ë“¤ì„ ë§ì”€í•´ì£¼ì„¸ìš”!"):
    with st.chat_message("user"):
        st.write(user_question)
    st.session_state.message_list.append({"role": "user", "content": user_question})

    with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤"):
        ai_message = get_ai_message(user_question)
        with st.chat_message("ai"):
            st.write(ai_message)
        st.session_state.message_list.append({"role": "ai", "content": ai_message})
